{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyngrok","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install nvidia-cuda-toolkit -y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!make server -C /kaggle/working/llama.cpp LLAMA_CUDA=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli download Qwen/CodeQwen1.5-7B-Chat-GGUF codeqwen-1_5-7b-chat-q4_k_m.gguf --local-dir . --local-dir-use-symlinks False\n!mv /kaggle/working/codeqwen-1_5-7b-chat-q4_k_m.gguf /kaggle/working/llama.cpp/models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ngrokToken留空则使用localtunnel进行内网穿透\nngrokToken = \"2g5SoIqUMNlj08BlPCgAdQDA7cy_3wUF54dZjMfAJRgh4Mjgu\"\nuse_pinggy = True\n\nfrom huggingface_hub import hf_hub_download\nfrom pathlib import Path\n\nif ngrokToken:\n    from pyngrok import conf, ngrok\n    conf.get_default().auth_token = ngrokToken\n    conf.get_default().monitor_thread = False\n    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n    if len(ssh_tunnels) == 0:\n        ssh_tunnel = ngrok.connect(8080)\n        print('address：'+ssh_tunnel.public_url)\n    else:\n        print('address：'+ssh_tunnels[0].public_url)\nelif use_pinggy:\n    import subprocess\n    import threading\n    def start_pinggy(port):\n        cmd = f\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -p 443 -R0:localhost:{port} a.pinggy.io\"\n        p = subprocess.Popen(cmd.split(\" \"), stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_pinggy, daemon=True, args=(8080,)).start()\nelse:\n    import subprocess\n    import threading\n    def start_localtunnel(port):\n        p = subprocess.Popen([\"lt\", \"--port\", f\"{port}\"], stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_localtunnel, daemon=True, args=(8080,)).start()\n\n\n\n!/kaggle/working/llama.cpp/server -m /kaggle/working/llama.cpp/models/codeqwen-1_5-7b-chat-q4_k_m.gguf -c 2048 -ngl 10000","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:15:27.529570Z","iopub.execute_input":"2024-05-18T03:15:27.529985Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"address：https://c7bc-34-80-242-244.ngrok-free.app\n{\"tid\":\"139557030264832\",\"timestamp\":1716002129,\"level\":\"INFO\",\"function\":\"main\",\"line\":2942,\"msg\":\"build info\",\"build\":2918,\"commit\":\"05834841\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002129,\"level\":\"INFO\",\"function\":\"main\",\"line\":2947,\"msg\":\"system info\",\"n_threads\":2,\"n_threads_batch\":-1,\"total_threads\":4,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \"}\nllama_model_loader: loaded meta data with 23 key-value pairs and 387 tensors from /kaggle/working/llama.cpp/models/codeqwen-1_5-7b-chat-q4_k_m.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = CodeQwen-7B-AWQ-fp16\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 32\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 65536\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 13440\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,92416]   = [\"<unk>\", \"<s>\", \"<|endoftext|>\", \"<|...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,92416]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,92416]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 4\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 92298\nllama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  161 tensors\nllama_model_loader: - type q5_0:   16 tensors\nllama_model_loader: - type q8_0:   16 tensors\nllama_model_loader: - type q4_K:  177 tensors\nllama_model_loader: - type q6_K:   17 tensors\nllm_load_vocab: mismatch in special tokens definition ( 749/92416 vs 407/92416 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 92416\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 65536\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 8\nllm_load_print_meta: n_embd_k_gqa     = 512\nllm_load_print_meta: n_embd_v_gqa     = 512\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 13440\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 65536\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 7.25 B\nllm_load_print_meta: model size       = 4.41 GiB (5.23 BPW) \nllm_load_print_meta: general.name     = CodeQwen-7B-AWQ-fp16\nllm_load_print_meta: BOS token        = 2 '<|endoftext|>'\nllm_load_print_meta: EOS token        = 4 '<|im_end|>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 92298 '<fim_pad>'\nllm_load_print_meta: LF token         = 1396 '<0x0A>'\nllm_load_print_meta: EOT token        = 4 '<|im_end|>'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   203.06 MiB\nllm_load_tensors:      CUDA0 buffer size =  4314.04 MiB\n........................................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 1000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   128.00 MiB\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.71 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   188.50 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\nllama_new_context_with_model: graph nodes  = 1126\nllama_new_context_with_model: graph splits = 2\n{\"tid\":\"139557030264832\",\"timestamp\":1716002131,\"level\":\"INFO\",\"function\":\"init\",\"line\":716,\"msg\":\"initializing slots\",\"n_slots\":1}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002131,\"level\":\"INFO\",\"function\":\"init\",\"line\":725,\"msg\":\"new slot\",\"id_slot\":0,\"n_ctx_slot\":2048}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002131,\"level\":\"INFO\",\"function\":\"main\",\"line\":3042,\"msg\":\"model loaded\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002131,\"level\":\"INFO\",\"function\":\"main\",\"line\":3064,\"msg\":\"chat template\",\"chat_example\":\"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHi there<|im_end|>\\n<|im_start|>user\\nHow are you?<|im_end|>\\n<|im_start|>assistant\\n\",\"built_in\":true}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002131,\"level\":\"INFO\",\"function\":\"main\",\"line\":3795,\"msg\":\"HTTP server listening\",\"n_threads_http\":\"3\",\"port\":\"8080\",\"hostname\":\"127.0.0.1\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002131,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002215,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1044,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":0}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002215,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2093,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":0,\"p0\":0}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002216,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":315,\"msg\":\"prompt eval time     =     235.86 ms /    10 tokens (   23.59 ms per token,    42.40 tokens per second)\",\"id_slot\":0,\"id_task\":0,\"t_prompt_processing\":235.858,\"n_prompt_tokens_processed\":10,\"t_token\":23.5858,\"n_tokens_second\":42.39839225296577}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002216,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":331,\"msg\":\"generation eval time =     704.98 ms /    15 runs   (   47.00 ms per token,    21.28 tokens per second)\",\"id_slot\":0,\"id_task\":0,\"t_token_generation\":704.981,\"n_decoded\":15,\"t_token\":46.998733333333334,\"n_tokens_second\":21.27716917193513}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002216,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":342,\"msg\":\"          total time =     940.84 ms\",\"id_slot\":0,\"id_task\":0,\"t_prompt_processing\":235.858,\"t_token_generation\":704.981,\"t_total\":940.8389999999999}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002216,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1787,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":0,\"n_ctx\":2048,\"n_past\":24,\"n_system_tokens\":0,\"n_cache_tokens\":0,\"truncated\":false}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002216,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139556731772928\",\"timestamp\":1716002216,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2889,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":53194,\"status\":200,\"method\":\"POST\",\"path\":\"/chat/completions\",\"params\":{}}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002216,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002229,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1044,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":17}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002229,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2093,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":17,\"p0\":0}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002237,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":315,\"msg\":\"prompt eval time     =     285.76 ms /    39 tokens (    7.33 ms per token,   136.48 tokens per second)\",\"id_slot\":0,\"id_task\":17,\"t_prompt_processing\":285.763,\"n_prompt_tokens_processed\":39,\"t_token\":7.3272564102564095,\"n_tokens_second\":136.4767307174127}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002237,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":331,\"msg\":\"generation eval time =    7544.68 ms /   150 runs   (   50.30 ms per token,    19.88 tokens per second)\",\"id_slot\":0,\"id_task\":17,\"t_token_generation\":7544.677,\"n_decoded\":150,\"t_token\":50.297846666666665,\"n_tokens_second\":19.881566831820635}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002237,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":342,\"msg\":\"          total time =    7830.44 ms\",\"id_slot\":0,\"id_task\":17,\"t_prompt_processing\":285.763,\"t_token_generation\":7544.677,\"t_total\":7830.44}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002237,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1787,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":17,\"n_ctx\":2048,\"n_past\":188,\"n_system_tokens\":0,\"n_cache_tokens\":0,\"truncated\":false}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002237,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139556723380224\",\"timestamp\":1716002237,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2889,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60866,\"status\":200,\"method\":\"POST\",\"path\":\"/chat/completions\",\"params\":{}}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002237,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002407,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1044,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":169}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002407,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2093,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":169,\"p0\":0}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002409,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":315,\"msg\":\"prompt eval time     =     811.23 ms /   324 tokens (    2.50 ms per token,   399.39 tokens per second)\",\"id_slot\":0,\"id_task\":169,\"t_prompt_processing\":811.234,\"n_prompt_tokens_processed\":324,\"t_token\":2.5038086419753087,\"n_tokens_second\":399.39154424000964}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002409,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":331,\"msg\":\"generation eval time =    1074.32 ms /    22 runs   (   48.83 ms per token,    20.48 tokens per second)\",\"id_slot\":0,\"id_task\":169,\"t_token_generation\":1074.323,\"n_decoded\":22,\"t_token\":48.83286363636364,\"n_tokens_second\":20.478012664720012}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002409,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":342,\"msg\":\"          total time =    1885.56 ms\",\"id_slot\":0,\"id_task\":169,\"t_prompt_processing\":811.234,\"t_token_generation\":1074.323,\"t_total\":1885.5570000000002}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002409,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1787,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":169,\"n_ctx\":2048,\"n_past\":345,\"n_system_tokens\":0,\"n_cache_tokens\":0,\"truncated\":false}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002409,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139556036796416\",\"timestamp\":1716002409,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2889,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":57016,\"status\":200,\"method\":\"POST\",\"path\":\"/chat/completions\",\"params\":{}}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002409,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002421,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1044,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":193}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002421,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2093,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":193,\"p0\":0}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002423,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":315,\"msg\":\"prompt eval time     =     790.82 ms /   273 tokens (    2.90 ms per token,   345.21 tokens per second)\",\"id_slot\":0,\"id_task\":193,\"t_prompt_processing\":790.815,\"n_prompt_tokens_processed\":273,\"t_token\":2.896758241758242,\"n_tokens_second\":345.2134822935831}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002423,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":331,\"msg\":\"generation eval time =     817.17 ms /    17 runs   (   48.07 ms per token,    20.80 tokens per second)\",\"id_slot\":0,\"id_task\":193,\"t_token_generation\":817.167,\"n_decoded\":17,\"t_token\":48.06864705882353,\"n_tokens_second\":20.80358115293447}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002423,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":342,\"msg\":\"          total time =    1607.98 ms\",\"id_slot\":0,\"id_task\":193,\"t_prompt_processing\":790.815,\"t_token_generation\":817.167,\"t_total\":1607.982}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002423,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1787,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":193,\"n_ctx\":2048,\"n_past\":289,\"n_system_tokens\":0,\"n_cache_tokens\":0,\"truncated\":false}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002423,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139556731772928\",\"timestamp\":1716002423,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2889,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":32982,\"status\":200,\"method\":\"POST\",\"path\":\"/chat/completions\",\"params\":{}}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002423,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002582,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1044,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":212}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002582,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2093,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":212,\"p0\":0}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002586,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":315,\"msg\":\"prompt eval time     =     584.29 ms /   220 tokens (    2.66 ms per token,   376.52 tokens per second)\",\"id_slot\":0,\"id_task\":212,\"t_prompt_processing\":584.293,\"n_prompt_tokens_processed\":220,\"t_token\":2.6558772727272726,\"n_tokens_second\":376.5234223240737}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002586,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":331,\"msg\":\"generation eval time =    3462.28 ms /    69 runs   (   50.18 ms per token,    19.93 tokens per second)\",\"id_slot\":0,\"id_task\":212,\"t_token_generation\":3462.279,\"n_decoded\":69,\"t_token\":50.177956521739134,\"n_tokens_second\":19.929069840992018}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002586,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":342,\"msg\":\"          total time =    4046.57 ms\",\"id_slot\":0,\"id_task\":212,\"t_prompt_processing\":584.293,\"t_token_generation\":3462.279,\"t_total\":4046.572}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002586,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1787,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":212,\"n_ctx\":2048,\"n_past\":288,\"n_system_tokens\":0,\"n_cache_tokens\":0,\"truncated\":false}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002586,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139556723380224\",\"timestamp\":1716002586,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2889,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49596,\"status\":200,\"method\":\"POST\",\"path\":\"/chat/completions\",\"params\":{}}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002586,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002650,\"level\":\"INFO\",\"function\":\"launch_slot_with_task\",\"line\":1044,\"msg\":\"slot is processing task\",\"id_slot\":0,\"id_task\":283}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002650,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":2093,\"msg\":\"kv cache rm [p0, end)\",\"id_slot\":0,\"id_task\":283,\"p0\":0}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002664,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":315,\"msg\":\"prompt eval time     =     815.04 ms /   372 tokens (    2.19 ms per token,   456.42 tokens per second)\",\"id_slot\":0,\"id_task\":283,\"t_prompt_processing\":815.043,\"n_prompt_tokens_processed\":372,\"t_token\":2.190975806451613,\"n_tokens_second\":456.4176368608773}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002664,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":331,\"msg\":\"generation eval time =   12957.68 ms /   252 runs   (   51.42 ms per token,    19.45 tokens per second)\",\"id_slot\":0,\"id_task\":283,\"t_token_generation\":12957.678,\"n_decoded\":252,\"t_token\":51.419357142857145,\"n_tokens_second\":19.447928865032765}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002664,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":342,\"msg\":\"          total time =   13772.72 ms\",\"id_slot\":0,\"id_task\":283,\"t_prompt_processing\":815.043,\"t_token_generation\":12957.678,\"t_total\":13772.721}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002664,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1787,\"msg\":\"slot released\",\"id_slot\":0,\"id_task\":283,\"n_ctx\":2048,\"n_past\":623,\"n_system_tokens\":0,\"n_cache_tokens\":0,\"truncated\":false}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002664,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139556731772928\",\"timestamp\":1716002664,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2889,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":51508,\"status\":200,\"method\":\"POST\",\"path\":\"/chat/completions\",\"params\":{}}\n{\"tid\":\"139557030264832\",\"timestamp\":1716002664,\"level\":\"INFO\",\"function\":\"update_slots\",\"line\":1813,\"msg\":\"all slots are idle\"}\n{\"tid\":\"139556036796416\",\"timestamp\":1716002796,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2889,\"msg\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":55400,\"status\":400,\"method\":\"POST\",\"path\":\"/chat/completions\",\"params\":{}}\n","output_type":"stream"}]}]}